---
layout: post
title: "DHThis and academic peer review"
date: 2013-09-11 15:39
comments: true
categories: [public scholarship]
share: true
tags:
- peer review
- publishing
- scholarship
- digital humanities
- open-access
---

[Yesterday's announcement](http://chronicle.com/blogs/profhacker/crowdsourcing-the-best-digital-humanities-content/52135) on Twitter and the Chronicle of Higher Education about the new crowdsourced digital humanities publication, [DHThis.org](http://dhthis.org), created quite a stir. Though there has been talk of a site like this for some time—a "slashdot or reddit for DH"—the announcement comes in the midst of a debate over issues of transparency and open peer review in academic publishing, precipitated by [a recent conflict](http://www.adelinekoh.org/blog/2013/08/29/journalofdigitalhumanitie/) regarding the Journal of Digital Humanities. Both praise and [critique](http://blog.whitneyannetrettien.com/2013/09/a-gentle-critique-of-dhthis.html) have come to DHThis very quickly, likely in part because of the recent debates. However, I will leave that debate and the question of transparency aside. What I'm interested in is the contribution that DHThis can make to recent discussions about academic peer review. DHThis takes an approach that I think can serve a lot of academic communities well: using crowdsourcing as a means of finding and discussing good content on the open web. However, some things are missing from DHThis—not things which diminish its usefulness, but things which make it incomplete as a means of academic peer review. And as long as the discussions center around crowdsourcing and its (in)ability to function as an effective and fair gatekeeper for academic discourse, we are going to miss a key component of the discussion of how to improve academic peer review.

Academic peer review does two things: it filters out sub-standard content in order to present only high-quality scholarship, and it helps authors make their scholarly articles better. The former tends to get the greater emphasis, both in practice and in discussions about how peer review can or should progress in the twenty-first century. However, my experiences writing for [*Hybrid Pedagogy*](http://hybridpedagogy.com) and writing and editing [*Engaging Students: Essays in Music Pedagogy*](http://flipcamp.org/engagingstudents) have impressed upon me the importance of the latter.

Because crowdsourcing directly affects what's in and what's out, the discussions about crowdsourcing in academic publishing that I've come across tend to address the appropriateness of crowdsourcing as a gatekeeper: Does it "catch the good" from any source or make it more difficult to find good scholarship outside already accepted bounds? Does it increase transparency in academic review or simply add to the confusion (particularly when an editor steps in to "correct" the issues that arise when the crowd is in control)? Etc. These are real and important questions, and I've experienced the significance of these issues myself, both when [a blog post of mine](http://kris.shaffermusic.com/2013/01/harmonic-syntax-in-corpus-studies/) was republished on [DHNow](http://digitalhumanitiesnow.org), and when I served as coordinating editor for *Engaging Students*. But crowdsourcing also has a huge impact on how academic peer review can increase the quality of scholarship and scholarly writings.

How can this be? Why would a website that mainly provides links to offsite content, along with an ability to vote up/down or leave comments, help make writing better?

This past summer, I wrote two essays for *Hybrid Pedagogy*. I found their review process refreshing and exhilarating (see my [blog post](http://kris.shaffermusic.com/2013/05/a-new-way-to-do-peer-review/) on this process). What made their review process such a positive experience for me as a writer was the emphasis in improving my writing. This emphasis was possible because *they accept/reject articles at the beginning of the review process*. Only after an article is accepted and a publication date assigned do reviewers step in and collaborate with the author in a non-anonymous back-and-forth in a Google document and video chat. Pre-acceptance makes anonymity unnecessary, takes psychological pressure off the author, and the low-pressure, quasi-in-person arrangement makes for a positive, collaborative experience, and ultimately a better article. I wish more academic publications did that (and we tried to emulate that as much as possible when working on *Engaging Students*).

Curation sites such as DHThis involve absolutely no collaborative back-and-forth, and the onus is entity on the author to improve their work based on the comments they receive (if they even see them, as discussion can happen both on the original site and on DHThis). However, sites like DHThis can function as a *pre-acceptance mechanism* for further projects. In fact, they are very well suited to the task.

Just as the [*Journal of Digital Humanities*](http://journalofdigitalhumanities.org) mines the cream that rises to the top of DHNow (which uses a process different from DHThis to determine what rises to the top) to find articles for many of its issues, a journal editor (or editorial board) can use sites like DHNow and DHThis as a source of scholarly writing that has already been found to have value for some portion of the scholarly community. Rather than accepting submissions and writing many rejection letters, and rather than putting reviewer efforts into writing detailed reports on articles that have little merit to warrant publication, editors can find the writings in a curation site that generate community interest, choose those that match the vision of the journal, and contact the author and reviewers to initiate a collaborative review process that is *guaranteed* publication at the end.

Of course, the idea of only working on articles that are guaranteed acceptance is wonderful, as is the idea of evaluating the merits of many articles, good and bad, without writing a single rejection letter. But that is only possible if there is good material to find, and if there is a way to find it. Crowdsourced ventures like DHThis can serve as great *starting points* that allow readers to find interesting things, and that allow academic editors to find work that is worthy of inclusion in their journal.

If the DHThis front page is an *end point*, a lot of work will need to go into refining the process of how submitted articles wind up on the front page. That will lead to greater complication, which decreases practical transparency—i.e., the process is opaque because it is complicated. But as experimentation with this process inevitably leads to some anomalous results, editors will invariably need to step in and "correct" the system lest it lose its value, which further decreases the transparency of a process meant to be community-driven and open. On the other hand, if the DHThis front page is meant as a *starting point* for those seeking good academic content—both readers and editors—it need not be perfect, and there is less pressure to "correct" anomalies that arise from the open process.

For an article on its way to journal publication, then, the process is transparent in the crowdsourced, "catch the good" stage as well as the stage of collaboration with reviewers (should we even call them that?) to increase the quality of the writing. The only opaque phase—where editors select articles for journal or edited volume inclusion—is the one at which there is no expectation from the author, who submitted it nowhere.

I'm excited where DHThis, and more importantly the publication models it encourages for others, may take us in terms of academic peer review. We desperately need new models of academic publishing and peer review (not to mention the promotion and tenure evaluation process that is integrally tied to academic publishing), or at least new ideas that can lead to new models. But we won't make it far if we only think about new tools as new ways to keep the gate. Peer review should be about more than keeping the bad out; it should be about making the good better. Let's keep both of those roles in the discussion of any new tools that emerge.
